## Lab 1: Foundations & Perceptrons

This assignment focuses on the fundamentals of Deep Learning, covering tensor manipulation in PyTorch and TensorFlow, as well as the implementation of basic neural architectures.

### Assignment Details
* **Questions:** `Assignment1.pdf`
* **Solutions:** `U23AI006_DL_Lab1.ipynb`

### Key Topics Covered

#### 1. PyTorch Tensors & Autograd
* **Tensors:** Initialization methods, data types, and memory management.
* **Operations:** Arithmetic, broadcasting, indexing, and reshaping.
* **Autograd:** Exploring automatic differentiation and computational graphs.

#### 2. TensorFlow Linear Algebra
* Implementation of core linear algebra operations (Matrix multiplication, transposition, etc.) using the TensorFlow framework.

#### 3. Perceptron Logic Gates
* Implementing **AND** and **OR** gates using a single-layer Perceptron.
* Demonstrating linear separability in logic gates.

#### 4. XOR Problem (MLP)
* Implementation of the **XOR** gate using a Multi-Layer Perceptron (MLP) in PyTorch.
* Understanding why non-linear activation functions and hidden layers are required for non-linearly separable data.


#### 5. Regression Neural Network
* Building a simple feed-forward neural network to solve a continuous regression problem.
* Using loss functions (MSE) and optimizers to minimize prediction error.